{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to SCoPe and its periodic variables: ZTF Summer School\n",
    "\n",
    "### Keaton Bell <keatonb@uw.edu>, Michael Coughlin <cough052@umn.edu>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to frequency analysis of variable light curves in the ZTF variable source catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will provide a brief introduction to frequency analysis of variable star light curves, like those being obtained from ZTF.  We will demonstrate how to obtain a multi-sinusoid fit to time series data, how to deal with potential challenges from gaps in the data, and we'll conclude with a discussion of signal significance.  We will use the packages [astroquery](https://astroquery.readthedocs.io/en/latest/), [ztfquery](https://github.com/MickaelRigault/ztfquery), [lightkurve](http://docs.lightkurve.org/), and [Pyriod](https://github.com/keatonb/Pyriod/), so you'll need those installed, and you'll also need an IRSA account to download ZTF data.\n",
    "\n",
    "After you install Pyriod, make sure to enable the following Jupyter extensions in the terminal (before you start your Jupyter Notebook server!):\n",
    "```\n",
    "jupyter nbextension enable --py --sys-prefix qgrid\n",
    "jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "jupyter nbextension enable --py --sys-prefix ipympl\n",
    "```\n",
    "\n",
    "Pyriod works in Jupyter Notebooks, but not in Jupyter Lab. The widgets require the `%matplotlib widget` magic command to be called.\n",
    "\n",
    "First, since we want to analyze variable light curves, we'll use SCoPe published in [van Roestel et al. (2021, AJ, 161 267)](https://iopscience.iop.org/article/10.3847/1538-3881/abe853/meta). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cannot change to a different GUI toolkit: widget. Using notebook instead.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DATA_PATH = './data/dataset.csv'\n",
    "df_raw_data = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_COL = [# 10 Cols for the phenomenological classifiers \n",
    "              'variable', 'periodic', 'long timescale', 'irregular', 'eclipsing', 'EA', 'EB', 'EW', 'flaring', 'bogus',\n",
    "              # new\n",
    "              'non-variable', 'dipping', 'blend', 'bright star', 'ccd artifact', 'galaxy', 'sinusoidal', 'sawtooth', \n",
    "              'elliptical',\n",
    "    \n",
    "    \n",
    "              # 13 Cols for the ontological classifiers\n",
    "              'pulsator', 'Delta Scu', 'Cepheid', 'RR Lyrae', 'LPV', 'Mira', 'SRV', 'binary star', 'W Uma', 'Beta Lyr',\n",
    "              'RS CVn', 'AGN', 'YSO',\n",
    "              # new\n",
    "              'F', 'O', 'Cepheid type-II', 'detached eclipsing MS-MS', 'compact binary', 'eclipsing WD+dM (NN Ser)',\n",
    "              'eclipsing sdB+dM (HW Vir)', 'RR Lyrae Blazhko', 'RR Lyrae ab', 'RR Lyrae c', 'RR Lyrae d', 'BL Her', \n",
    "              'RV Tau', 'W Virginis',  \n",
    "    \n",
    "              # unclear:\n",
    "              'double period', 'half period', 'multi periodic', 'nice', 'niice', 'wrong period',\n",
    "             ]\n",
    "\n",
    "FEATS_COL = [# 38 Cols for the phenomenological classifiers \n",
    "             'period', 'significance', 'n', 'median', 'wmean', 'wstd', 'chi2red', 'roms', 'norm_peak_to_peak_amp',\n",
    "             'norm_excess_var', 'median_abs_dev', 'iqr', 'f60', 'f70', 'f80' , 'f90', 'skew', 'smallkurt', \n",
    "             'inv_vonneumannratio', 'welch_i', 'stetson_j', 'stetson_k', 'ad', 'sw', 'f1_power', 'f1_bic','f1_amp', \n",
    "             'f1_phi0', 'f1_relamp1', 'f1_relphi1', 'f1_relamp2', 'f1_relphi2', 'f1_relamp3', 'f1_relphi3', 'f1_relamp4', \n",
    "             'f1_relphi5', 'n_ztf_alerts', 'mean_ztf_alert_braai',  \n",
    "             \n",
    "             # 30 Cols for the ontological classifiers\n",
    "             'AllWISE__w1mpro', 'AllWISE__w1sigmpro', 'AllWISE__w2mpro', 'AllWISE__w2sigmpro', 'AllWISE__w3mpro',\n",
    "             'AllWISE__w3sigmpro', 'AllWISE__w4mpro','AllWISE__w4sigmpro', 'AllWISE__ph_qual',\n",
    "             'Gaia_DR2__phot_g_mean_mag', 'Gaia_DR2__phot_bp_mean_mag', 'Gaia_DR2__phot_rp_mean_mag', 'Gaia_DR2__parallax',\n",
    "             'Gaia_DR2__parallax_error', 'Gaia_DR2__pmra', 'Gaia_DR2__pmra_error', 'Gaia_DR2__pmdec', 'Gaia_DR2__pmdec_error',\n",
    "             'Gaia_DR2__astrometric_excess_noise', 'Gaia_DR2__phot_bp_rp_excess_factor',\n",
    "             'PS1_DR1__gMeanPSFMag', 'PS1_DR1__gMeanPSFMagErr', 'PS1_DR1__rMeanPSFMag', 'PS1_DR1__rMeanPSFMagErr', \n",
    "             'PS1_DR1__iMeanPSFMag', 'PS1_DR1__iMeanPSFMagErr', 'PS1_DR1__zMeanPSFMag', 'PS1_DR1__zMeanPSFMagErr', \n",
    "             'PS1_DR1__yMeanPSFMag', 'PS1_DR1__yMeanPSFMagErr', 'PS1_DR1__qualityFlag']\n",
    "\n",
    "\n",
    "df_feats = df_raw_data[FEATS_COL]\n",
    "df_labels = df_raw_data[LABELS_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra, dec = df_raw_data['ra'], df_raw_data['dec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find number of objects of each type\n",
    "\n",
    "NB: an object label can be 0, 0.25, 0.5, 0.75, or 1 depending on the labelers'certainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable 44054.710999999996\n",
      "periodic 36988.5\n",
      "long timescale 523.75\n",
      "irregular 948.8749999999999\n",
      "eclipsing 25089.625\n",
      "EA 459.0\n",
      "EB 524.5\n",
      "EW 21581.625\n",
      "flaring 450.75\n",
      "bogus 784.625\n",
      "non-variable 25592.664\n",
      "dipping 43.5\n",
      "blend 227.25\n",
      "bright star 178.0\n",
      "ccd artifact 17.5\n",
      "galaxy 163.75\n",
      "sinusoidal 628.0\n",
      "sawtooth 577.0\n",
      "elliptical 524.0\n",
      "pulsator 10983.875\n",
      "Delta Scu 3459.0\n",
      "Cepheid 523.75\n",
      "RR Lyrae 6573.0\n",
      "LPV 353.75\n",
      "Mira 90.75\n",
      "SRV 242.75\n",
      "binary star 25030.0\n",
      "W Uma 21514.25\n",
      "Beta Lyr 457.5\n",
      "RS CVn 665.0\n",
      "AGN 344.5\n",
      "YSO 425.9583333333333\n",
      "F 14.0\n",
      "O 0.0\n",
      "Cepheid type-II 72.5\n",
      "detached eclipsing MS-MS 2077.0\n",
      "compact binary 7.5\n",
      "eclipsing WD+dM (NN Ser) 6.5\n",
      "eclipsing sdB+dM (HW Vir) 0.5\n",
      "RR Lyrae Blazhko 54.5\n",
      "RR Lyrae ab 1870.0\n",
      "RR Lyrae c 2567.0\n",
      "RR Lyrae d 205.0\n",
      "BL Her 14.0\n",
      "RV Tau 0.5\n",
      "W Virginis 12.75\n",
      "double period 435.5\n",
      "half period 166.0\n",
      "multi periodic 125.25\n",
      "nice 21.0\n",
      "niice 5.0\n",
      "wrong period 742.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for column in df_labels:\n",
    "    print(column, np.sum(df_labels[column]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose an object from the table, and let's download and analyze the data.  Try a random one or browse the table for something that interests you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ind = np.random.choice(np.arange(len(df_raw_data))) #Random row number\n",
    "#ind = 368056 #For tutorial video\n",
    "row = df_raw_data.iloc[ind]\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need an IRSA account to access the data.  Download and display the available data as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ztfquery import lightcurve\n",
    "lcq = lightcurve.LCQuery()\n",
    "result = lcq.query_position(row[\"ra\"], row[\"dec\"], 3)\n",
    "data = result.data\n",
    "result.show_lc() #Display a light curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Jupyter widget could not be displayed because the widget state could not be found. This could happen if the kernel storing the widget is no longer available, or if the widget state was not saved in the notebook. You may be able to create the widget by running the appropriate cells.\n",
    "\n",
    "We'll analyze data taken in a single filter in this tutorial, so pick one that has plenty of data in the plot above. The filter codes are `zg` (green above), `zr` (red), and `zi` (orange).  We will also reject any points that have corresponding catalog flags indicating potentially poor quality data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtercode = 'zr' #just this filter\n",
    "catflags = 0 #reliable data only (hopefully)\n",
    "datatable = data[(data.filtercode == filtercode) & (data.catflags == catflags)]\n",
    "\n",
    "#Look at the contents of the resulting table\n",
    "datatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's all the time series data for our chosen filter.  We need to put this in a form that Pyriod will understand, i.e., the `lightkurve` package's `LightCurve` object.  But before that, we'll want to convert from magnitude to flux units for our analysis (analyzing magnitudes is okay, this is just my preference).  We'll normalize the light curves (to be centered around 1.0), so we don't need to worry about the flux zero point, so I just use some arbitrary reference magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datatable['mjd']\n",
    "flux = 10**(-0.4*(datatable['mag']-21))\n",
    "flux_err = flux * datatable['magerr'] / 1.1816\n",
    "\n",
    "import lightkurve as lk\n",
    "lc = lk.LightCurve(time = time, flux = flux, flux_err = flux_err)\n",
    "lc = lc.normalize() #normalize so centered on 1\n",
    "lc.scatter() #plot the light curve data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Jupyter widget could not be displayed because the widget state could not be found. This could happen if the kernel storing the widget is no longer available, or if the widget state was not saved in the notebook. You may be able to create the widget by running the appropriate cells.\n",
    "\n",
    "Okay, our light curve is now in units of relative flux.  We're finally ready to pass this on to Pyriod's interactive frequency analysis tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pyriod import Pyriod\n",
    "pyriod = Pyriod(lc, freq_unit='day', maxfreq = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyriod has four interactive \"cells\" that let you interact with the data and obtain a multi-sinusoid fit.  They are the TimeSeries, Periodogram, Signals, and Log cells, each displayed in the next four cells. Click the \"info\" drop-down for more information about how to interact with each cell. A demonstration of how to use these tools to obtain a reliable fit, watch the accompanying video.  If you want to develop a better understanding of the Lomb-Scargle periodogram, check out [Vanderplas (2018, ApJS, 236, 16)](https://ui.adsabs.harvard.edu/abs/2018ApJS..236...16V)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyriod.TimeSeries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyriod.Periodogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyriod.Signals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyriod.Log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to those tools, we need some way of assessing the significance of peaks in the periodgram so we can decide whether to concorporate them into our analysis.  We'll use a technique called bootstrapping, where we make the assumption that the residuals contain only uncorrelated noise to see how high of a periodogram peak such noise could reasonably produce.  We treat the data as noise by drawing randomly from the light curve new \"noisy\" light curves that are sampled at the same times as the original, destroying any coherent signals.  If we find higher peaks in the periodogram of our data, we can trust to some level of confidence that it corresponds to a real underlying signal.  That doesn't mean the highest peak is at the frequency of variability!  A different test that injects signals into the bootstrapped noise model and checks how often the highest peak corresponds to the injected frequency can test for this criterion.  Also beware of a heightened noise level at low frequencies, which can be due to long-timescale noise in the data. It's often a better idea to determine a significance threshold as a multiple of the local mean periodogram values. Five times the local mean level in the periodogram will probably yield reliable pleaks.  This process can take hours, but it's worth it to recalculate a new significance threshold when there are no longer peaks above the previously calculated value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm #status bar\n",
    "fap = 0.01 #False alarm probability\n",
    "nruns = 100\n",
    "highestpeaks = np.zeros(nruns)\n",
    "noisedistribution = 1 + pyriod.lc.resid[np.where(pyriod.lc[\"mask\"])].value\n",
    "\n",
    "for i in tqdm(range(nruns)): #~3 hours\n",
    "    bootstrappedlc = lc.copy()\n",
    "    bootstrappedlc.flux = np.random.choice(noisedistribution,len(lc)) #with replacement\n",
    "    bootstrappedlc = bootstrappedlc.normalize() #re-normalize\n",
    "    per = bootstrappedlc.to_periodogram(normalization='amplitude',freq_unit=pyriod.freq_unit,\n",
    "                                        frequency=pyriod.freqs)*pyriod.amp_conversion\n",
    "    highestpeaks[i] = per.max_power.value\n",
    "    \n",
    "#print the peak height corresponding to the desired False Alarm Probability\n",
    "np.percentile(highestpeaks,100*(1-fap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
